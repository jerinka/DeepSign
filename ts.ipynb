{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ts.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THY0c2nLHXtM","executionInfo":{"status":"ok","timestamp":1616816696970,"user_tz":-330,"elapsed":5148,"user":{"displayName":"DHANYA PAULOSE","photoUrl":"","userId":"09776723268532932548"}},"outputId":"4c5ea3cc-2096-4447-e330-4eb833865db1"},"source":["import os\n","import re\n","import cv2\n","import glob\n","import h5py\n","import numpy as np\n","import imgaug as ia\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import xml.etree.ElementTree as ET\n","import matplotlib.patches as patches\n","from imgaug import augmenters as iaa\n","import tensorflow.keras.backend as K\n","print('Tensorflow version : {}'.format(tf.__version__))\n","print('GPU : {}'.format(tf.config.list_physical_devices('GPU')))\n","import tensorflow.keras.backend as K\n","from keras.engine import Layer, InputSpec\n","from tensorflow.keras.layers import Concatenate, concatenate, Dropout, LeakyReLU, Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tensorflow version : 2.4.1\n","GPU : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EaeqBseSFqhX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9eWV46Zk0meu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616816802248,"user_tz":-330,"elapsed":618,"user":{"displayName":"DHANYA PAULOSE","photoUrl":"","userId":"09776723268532932548"}},"outputId":"247aa4d2-9aa7-467e-81d9-d8daf52f51d8"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HFjy9CI3JlKf","executionInfo":{"status":"ok","timestamp":1616816807654,"user_tz":-330,"elapsed":1139,"user":{"displayName":"DHANYA PAULOSE","photoUrl":"","userId":"09776723268532932548"}}},"source":["root_path = '/content/gdrive/My Drive/Dhanya/'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3pvmYSQi_za"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ftvd7zzzIQr4"},"source":["def parse_annotation(dir ,labels):\n","    '''\n","    Parse XML files in PASCAL VOC format.\n","    \n","    Parameters\n","    ----------\n","    - ann_dir : annotations files directory\n","    - img_dir : images files directory\n","    - labels : labels list\n","    \n","    Returns\n","    -------\n","    - imgs_name : numpy array of images files path (shape : images count, 1)\n","    - true_boxes : numpy array of annotations for each image (shape : image count, max annotation count, 5)\n","        annotation format : xmin, ymin, xmax, ymax, class\n","        xmin, ymin, xmax, ymax : image unit (pixel)\n","        class = label index\n","    '''\n","    \n","    max_annot = 0\n","    imgs_name = []\n","    annots = []\n","    \n","    # Parse file\n","    for ann in sorted(os.listdir(dir)):\n","      img_url = \"\"\n","      if(ann[-3:]==\"xml\"):\n","    \n","        annot_count = 0\n","        boxes = []\n","        tree = ET.parse(dir + ann)\n","        for elem in tree.iter():  \n","            if 'filename' in elem.tag:\n","                imgs_name.append(dir + elem.text)\n","                img_url = dir + elem.text\n","            if 'width' in elem.tag:\n","                w = int(elem.text)\n","            if 'height' in elem.tag:\n","                h = int(elem.text)\n","            if 'object' in elem.tag or 'part' in elem.tag:                  \n","                box = np.zeros((5))\n","                for attr in list(elem):\n","                    if 'name' in attr.tag:\n","                        box[4] = labels.index(attr.text) + 1 # 0:label for no bounding box\n","                    if 'bndbox' in attr.tag:\n","                        annot_count += 1\n","                        for dim in list(attr):\n","                            if 'xmin' in dim.tag:\n","                                box[0] = int(round(float(dim.text)))\n","                            if 'ymin' in dim.tag:\n","                                box[1] = int(round(float(dim.text)))\n","                            if 'xmax' in dim.tag:\n","                                box[2] = int(round(float(dim.text)))\n","                            if 'ymax' in dim.tag:\n","                                box[3] = int(round(float(dim.text)))\n","                boxes.append(np.asarray(box))\n","        \n","        if w != IMAGE_W or h != IMAGE_H :\n","            img = cv2.imread(img_url)\n","            cv2.resize(img,(512,512),interpolation = cv2.INTER_AREA)\n","            w,h = 512,512\n","            \n","        annots.append(np.asarray(boxes))\n","        \n","\n","        if annot_count > max_annot:\n","            max_annot = annot_count\n","           \n","    # Rectify annotations boxes : len -> max_annot\n","    imgs_name = np.array(imgs_name)  \n","    true_boxes = np.zeros((imgs_name.shape[0], max_annot, 5))\n","    for idx, boxes in enumerate(annots):\n","        true_boxes[idx, :boxes.shape[0], :5] = boxes\n","        \n","    return imgs_name, true_boxes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDTYyFXyIVuY"},"source":["def parse_function(img_obj, true_boxes):\n","    x_img_string = tf.io.read_file(img_obj)\n","    x_img = tf.image.decode_png(x_img_string, channels=3) # dtype=tf.uint8\n","    x_img = tf.image.convert_image_dtype(x_img, tf.float32) # pixel value /255, dtype=tf.float32, channels : RGB\n","    return x_img, true_boxes\n","\n","def get_dataset(dir,  batch_size):\n","  \n","    imgs_name, bbox = parse_annotation(dir, LABELS)\n","    dataset = tf.data.Dataset.from_tensor_slices((imgs_name, bbox))    \n","    dataset = dataset.shuffle(len(imgs_name))\n","    dataset = dataset.repeat()\n","    dataset = dataset.map(parse_function, num_parallel_calls=6)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(10)\n","    print('-------------------')\n","    print('Dataset:')\n","    print('Images count: {}'.format(len(imgs_name)))\n","    print('Step per epoch: {}'.format(len(imgs_name) // batch_size))\n","    print('Images per epoch: {}'.format(batch_size * (len(imgs_name) // batch_size)))\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"20wLWVL2IYH1"},"source":["train_dataset = None\n","dir = root_path+\"lab/\"\n","\n","TRAIN_BATCH_SIZE = 10\n","VAL_BATCH_SIZE   = 10\n","LABELS           = ('Mandatory', 'Prohibitory', 'warning')\n","IMAGE_H, IMAGE_W = 512, 512\n","full_dataset= get_dataset(dir, TRAIN_BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHDjyMNBvD7m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpXuouuHIaCq"},"source":["total_size = 64\n","train_size = int(.7*total_size)\n","test_size = int(.15*total_size)\n","val_size = int(.15*total_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBLZSlSGI7wH"},"source":["type(full_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTr1D9x2I9cX"},"source":["train_dataset = full_dataset.shard(2, 0)\n","val_dataset = full_dataset.shard(2, 1) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAQ7JzaMI_Dp"},"source":["imgs_name, bbox = parse_annotation(dir, LABELS)\n","print(imgs_name.shape)\n","print(bbox.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkD7kNwBJDzg"},"source":["label_map={1:\"Mandatory\",2:\"Prohibitory\",3:\"Warning\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPpnz_hwJEUV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wYM-Z56AJIBH"},"source":["## Define the model"]},{"cell_type":"markdown","metadata":{"id":"FuM4sftoJUgd"},"source":["## Configuration set"]},{"cell_type":"code","metadata":{"id":"wYQtLkBDJFuS"},"source":["class Config:\n","\n","\tdef __init__(self):\n","\n","\t\t# Print the process or not\n","\t\tself.verbose = True\n","\n","\t\t# Name of base network\n","\t\tself.network = 'vgg'\n","\n","\t\t# Setting for data augmentation\n","\t\tself.use_horizontal_flips = False\n","\t\tself.use_vertical_flips = False\n","\t\tself.rot_90 = False\n","\n","\t\t# Anchor box scales\n","    # Note that if im_size is smaller, anchor_box_scales should be scaled\n","    # Original anchor_box_scales in the paper is [128, 256, 512]\n","\t\tself.anchor_box_scales = [64, 128, 256] \n","\n","\t\t# Anchor box ratios\n","\t\tself.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n","\n","\t\t# Size to resize the smallest side of the image\n","\t\t# Original setting in paper is 600. Set to 300 in here to save training time\n","\t\tself.im_size = 512\n","\n","\t\t\n","\n","\t\t# number of ROIs at once\n","\t\tself.num_rois = 4\n","\n","\t\t# stride at the RPN (this depends on the network configuration)\n","\t\tself.rpn_stride = 16\n","\n","\t\tself.balanced_classes = False\n","\n","\t\t# scaling the stdev\n","\t\tself.std_scaling = 4.0\n","\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n","\n","\t\t# overlaps for RPN\n","\t\tself.rpn_min_overlap = 0.3\n","\t\tself.rpn_max_overlap = 0.7\n","\n","\t\t# overlaps for classifier ROIs\n","\t\tself.classifier_min_overlap = 0.1\n","\t\tself.classifier_max_overlap = 0.5\n","\n","\t\t# placeholder for the class mapping, automatically generated by the parser\n","\t\tself.class_mapping = None\n","\n","\t\tself.model_path = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-FzmWPmGMOu0"},"source":["## VGG 16 model"]},{"cell_type":"code","metadata":{"id":"EpI6AQIzKrLd"},"source":["from keras.applications.vgg16 import VGG16\n","backbone_model = VGG16(weights=\"imagenet\",include_top=False)\n","for layers in backbone_model.layers[:-1]:\n","  layers.trainable=False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJlvppd3LKZu","outputId":"200ddeae-efef-4185-81c4-f40c08750fae"},"source":["backbone_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, None, None, 3)]   0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 0\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eico0UGbL3RH","outputId":"c0ad0b80-0f43-4159-fabd-66c85d9114d2"},"source":["sample_input=Input(shape=(512,512,3))\n","feature_map=backbone_model(sample_input)\n","print(feature_map.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(None, 16, 16, 512)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h0GR32UJMXTD"},"source":["## RPN MODULE"]},{"cell_type":"code","metadata":{"id":"Qw6cT0YkMAqF"},"source":["def rpn_layer(base_layers, num_anchors):\n","    \"\"\"Create a rpn layer\n","        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer\n","                Keep the padding 'same' to preserve the feature map's size\n","        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer\n","                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output\n","                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation\n","    Args:\n","        base_layers: vgg in here\n","        num_anchors: 9 in here\n","\n","    Returns:\n","        [x_class, x_regr, base_layers]\n","        x_class: classification for whether it's an object\n","        x_regr: bboxes regression\n","        base_layers: vgg in here\n","    \"\"\"\n","    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n","    print(x.shape)\n","\n","    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n","    print(x_class.shape)\n","    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n","    print(x_regr.shape)\n","    return [x_class, x_regr, base_layers]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQSLyBqfMS4B","outputId":"106320c6-f5e8-41cd-ff55-f5faba540f42"},"source":["rpn_layer(feature_map, num_anchors=9)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(None, 16, 16, 512)\n","(None, 16, 16, 9)\n","(None, 16, 16, 36)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<KerasTensor: shape=(None, 16, 16, 9) dtype=float32 (created by layer 'rpn_out_class')>,\n"," <KerasTensor: shape=(None, 16, 16, 36) dtype=float32 (created by layer 'rpn_out_regress')>,\n"," <KerasTensor: shape=(None, 16, 16, 512) dtype=float32 (created by layer 'vgg16')>]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"0IXKryFeMTB3"},"source":["class RoiPoolingConv(Layer):\n","    '''ROI pooling layer for 2D inputs.\n","    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n","    K. He, X. Zhang, S. Ren, J. Sun\n","    # Arguments\n","        pool_size: int\n","            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n","        num_rois: number of regions of interest to be used\n","    # Input shape\n","        list of two 4D tensors [X_img,X_roi] with shape:\n","        X_img:\n","        `(1, rows, cols, channels)`\n","        X_roi:\n","        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n","    # Output shape\n","        3D tensor with shape:\n","        `(1, num_rois, channels, pool_size, pool_size)`\n","    '''\n","    def __init__(self, pool_size, num_rois, **kwargs):\n","\n","        self.dim_ordering = K.image_dim_ordering()\n","        self.pool_size = pool_size\n","        self.num_rois = num_rois\n","\n","        super(RoiPoolingConv, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.nb_channels = input_shape[0][3]   \n","\n","    def compute_output_shape(self, input_shape):\n","        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n","\n","    def call(self, x, mask=None):\n","\n","        assert(len(x) == 2)\n","\n","        # x[0] is image with shape (rows, cols, channels)\n","        img = x[0]\n","\n","        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n","        rois = x[1]\n","\n","        input_shape = K.shape(img)\n","\n","        outputs = []\n","\n","        for roi_idx in range(self.num_rois):\n","\n","            x = rois[0, roi_idx, 0]\n","            y = rois[0, roi_idx, 1]\n","            w = rois[0, roi_idx, 2]\n","            h = rois[0, roi_idx, 3]\n","\n","            x = K.cast(x, 'int32')\n","            y = K.cast(y, 'int32')\n","            w = K.cast(w, 'int32')\n","            h = K.cast(h, 'int32')\n","\n","            # Resized roi of the image to pooling size (7x7)\n","            rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n","            outputs.append(rs)\n","                \n","\n","        final_output = K.concatenate(outputs, axis=0)\n","\n","        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n","        # Might be (1, 4, 7, 7, 3)\n","        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n","\n","        # permute_dimensions is similar to transpose\n","        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n","\n","        return final_output\n","    \n","    \n","    def get_config(self):\n","        config = {'pool_size': self.pool_size,\n","                  'num_rois': self.num_rois}\n","        base_config = super(RoiPoolingConv, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"asBXJJeBOOGL"},"source":["def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n","    \"\"\"Create a classifier layer\n","    \n","    Args:\n","        base_layers: vgg\n","        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n","        num_rois: number of rois to be processed in one time (4 in here)\n","\n","    Returns:\n","        list(out_class, out_regr)\n","        out_class: classifier layer output\n","        out_regr: regression layer output\n","    \"\"\"\n","\n","    input_shape = (num_rois,7,7,512)\n","\n","    pooling_regions = 7\n","\n","    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n","    # num_rois (4) 7x7 roi pooling\n","    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n","\n","    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n","    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n","    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n","    out = TimeDistributed(Dropout(0.5))(out)\n","    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n","    out = TimeDistributed(Dropout(0.5))(out)\n","\n","    # There are two output layer\n","    # out_class: softmax acivation function for classify the class name of the object\n","    # out_regr: linear activation function for bboxes coordinates regression\n","    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n","    # note: no regression target for bg class\n","    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n","\n","    return [out_class, out_regr]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_QevOB9vJRd"},"source":["train_image_folder = root_path+\"train/images/\"\n","train_annot_folder = root_path+\"train/labels/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"w1H8Jm5AvQI8","executionInfo":{"status":"error","timestamp":1616816449530,"user_tz":-330,"elapsed":1182,"user":{"displayName":"DHANYA PAULOSE","photoUrl":"","userId":"09776723268532932548"}},"outputId":"f93c2226-5d0d-4d51-aee9-106f2f9f11b6"},"source":["train_dataset = None\n","train_dataset= get_dataset(train_image_folder, train_annot_folder, LABELS, TRAIN_BATCH_SIZE)\n","\n","val_dataset = None\n","\n","val_dataset= get_dataset(train_image_folder, train_annot_folder, LABELS, VAL_BATCH_SIZE)\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-772d7c7c2057>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_annot_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'get_dataset' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"zo8wVrwkwsj5"},"source":[""]}]}